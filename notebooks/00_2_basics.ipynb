{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217f9e63",
   "metadata": {},
   "source": [
    "## Spark Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c574b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.float_format', lambda x : '{:,.2f}'.format(x))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b425eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ('sc' in locals() or 'sc' in globals()):\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('spark://spark-master:7077')\n",
    "    conf.set('spark.executor.memory', '512m')\n",
    "    conf.set('spark.app.name', 'basics')\n",
    "\n",
    "\n",
    "    sc = SparkContext.getOrCreate(SparkContext(conf=conf))\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d9b0f",
   "metadata": {},
   "source": [
    "<img src=\"../img/basics.png\" />\n",
    "\n",
    "### RDD\n",
    "\n",
    "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster. \n",
    "\n",
    "In other words, RDDs are a collection of objects similar to list in Python, with the difference being RDD is computed on several processes scattered across multiple physical servers also called nodes in a cluster while a Python collection lives and process in just one process.\n",
    "\n",
    "Additionally, RDDs provide data abstraction of partitioning and distribution of the data designed to run computations in parallel on several nodes, while doing transformations on RDD we don’t have to worry about the parallelism as PySpark by default provides.\n",
    "\n",
    "This Apache PySpark RDD tutorial describes the basic operations available on RDDs, such as map(), filter(), and persist() and many more. In addition, this tutorial also explains Pair RDD functions that operate on RDDs of key-value pairs such as groupByKey() and join() etc.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "__Immutability__\n",
    "PySpark RDD’s are immutable in nature meaning, once RDDs are created you cannot modify. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.\n",
    "\n",
    "__Fault Tolerance__\n",
    "PySpark operates on fault-tolerant data stores on HDFS, S3 e.t.c hence any RDD operation fails, it automatically reloads the data from other partitions. Also, When PySpark applications running on a cluster, PySpark task failures are automatically recovered for a certain number of times (as per the configuration) and finish the application seamlessly.\n",
    "\n",
    "__Lazy Evolution__\n",
    "PySpark does not evaluate the RDD transformations as they appear/encountered by Driver instead it keeps the all transformations as it encounters(DAG) and evaluates the all transformation when it sees the first RDD action.\n",
    "\n",
    "__Partitioning__\n",
    "When you create RDD from a data, It by default partitions the elements in a RDD. By default it partitions to the number of cores available.\n",
    "\n",
    "#### PySpark RDD Limitations\n",
    "\n",
    "PySpark RDDs are not much suitable for applications that make updates to the state store such as storage systems for a web application. For these applications, it is more efficient to use systems that perform traditional update logging and data checkpointing, such as databases. The goal of RDD is to provide an efficient programming model for batch analytics and leave these asynchronous applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f67355",
   "metadata": {},
   "source": [
    "### Create an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc19e09",
   "metadata": {},
   "source": [
    "<img src=\"../img/spark-partitions-2.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea9aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from parallelize    \n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "rdd = spark.sparkContext.parallelize(dataList)\n",
    "\n",
    "\n",
    "# Create RDD from external Data source\n",
    "rdd2 = spark.sparkContext.textFile(\"../data/hello_world.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6001df",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca866fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139aa76",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Transformations on Spark RDD returns another RDD and transformations are __lazy meaning they don’t execute until you call an action on RDD__. Some transformations on RDD’s are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return new RDD instead of updating the current.\n",
    "\n",
    "Also, RDD Transformations are Spark operations when executed on RDD, it results in a single or multiple new RDD’s. Since RDD are __immutable in nature__, transformations always create __new RDD__ without updating an existing one hence, this creates an RDD lineage.\n",
    "\n",
    "<img src=\"../img/linage.png\" width=\"500\"/>\n",
    "\n",
    "### RDD Transformations types\n",
    "\n",
    "#### Narrow Transformation\n",
    "\n",
    "Narrow transformations are the result of map() and filter() functions and these compute data that live on a single partition meaning there will not be any data movement between partitions to execute narrow transformations.\n",
    "\n",
    "<img src=\"../img/narrow-transformation.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc6d7d",
   "metadata": {},
   "source": [
    "#### Map\n",
    "\n",
    "Applies a given function to an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data,4)\n",
    "squared_rdd = rdd.map(lambda x:x**2)\n",
    "squared_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736929d",
   "metadata": {},
   "source": [
    "#### Filter\n",
    "\n",
    "Takes as input a condition and keeps only those elements that fulfill that condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "rdd = sc.parallelize(data,4)\n",
    "filtered_rdd = rdd.filter(lambda x:x%2==0)\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fed813",
   "metadata": {},
   "source": [
    "#### FlatMap\n",
    "\n",
    "Similar to map, but each input item can be mapped to 0 or more output items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55edec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4]\n",
    "rdd = sc.parallelize(data,4)\n",
    "flat_rdd = rdd.flatMap(lambda x:[x,x**3])\n",
    "flat_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897617b9",
   "metadata": {},
   "source": [
    "#### Wider Transformation\n",
    "Wider transformations are the result of groupByKey() and reduceByKey() functions and these compute data that live on many partitions meaning there will be data movements between partitions to execute wider transformations. Since these shuffles the data, they also called shuffle transformations.\n",
    "\n",
    "<img src=\"../img/wide-transformation.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd06318",
   "metadata": {},
   "source": [
    "#### Distinct\n",
    "\n",
    "Returns only distinct elements in an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d08ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,2,2,2,3,3,3,3,4,5,6,7,7,7,8,8,8,9,10]\n",
    "rdd = sc.parallelize(data,4)\n",
    "distinct_rdd = rdd.distinct()\n",
    "distinct_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f7981",
   "metadata": {},
   "source": [
    "#### Reduce By Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196dafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]\n",
    "rdd = sc.parallelize(data,4)\n",
    "\n",
    "category_price_rdd = rdd.map(lambda x: (x[1],x[2]))\n",
    "category_price_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_total_price_rdd = category_price_rdd.reduceByKey(lambda x,y:x+y)\n",
    "category_total_price_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef52260",
   "metadata": {},
   "source": [
    "#### Group By Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d25561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Apple','Fruit',200),('Banana','Fruit',24),('Tomato','Fruit',56),('Potato','Vegetable',103),('Carrot','Vegetable',34)]\n",
    "rdd = sc.parallelize(data,4)\n",
    "category_product_rdd = rdd.map(lambda x: (x[1],x[0]))\n",
    "category_product_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_products_by_category_rdd = category_product_rdd.groupByKey()\n",
    "findata = grouped_products_by_category_rdd.collect()\n",
    "for data in findata:\n",
    "    print(data[0],list(data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e5d3a",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "RDD actions are operations that return the raw values, In other words, any RDD function that returns other than RDD[T] is considered as an action in spark programming.\n",
    "\n",
    "As mentioned in RDD Transformations, all transformations are lazy meaning they do not get executed right away and action functions trigger to execute the transformations.\n",
    "\n",
    "### RDD Actions\n",
    "\n",
    "Narrow transformations are the result of map() and filter() functions and these compute data that live on a single partition meaning there will not be any data movement between partitions to execute narrow transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832cd3a",
   "metadata": {},
   "source": [
    "#### collect()\n",
    "\n",
    " It takes the whole RDD and brings it back to the driver program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724d534",
   "metadata": {},
   "source": [
    "#### take()\n",
    "Sometimes you will need to see what your RDD contains without getting all the elements in memory itself. take returns a list with the first n elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5147c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582eb453",
   "metadata": {},
   "source": [
    "#### takeOrdered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b349d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([5,3,12,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2565813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# descending order\n",
    "rdd.takeOrdered(3,lambda s:-1*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dbdcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ascending order\n",
    "rdd.takeOrdered(3,lambda s:1*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc182886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
