{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217f9e63",
   "metadata": {},
   "source": [
    "## Introduction to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c574b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.float_format', lambda x : '{:,.2f}'.format(x))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b425eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ('sc' in locals() or 'sc' in globals()):\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setMaster('spark://spark-master:7077')\n",
    "    conf.set('spark.executor.memory', '512m')\n",
    "    conf.set('spark.app.name', 'basics')\n",
    "\n",
    "\n",
    "    sc = SparkContext.getOrCreate(SparkContext(conf=conf))\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a898edd",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. \n",
    "\n",
    "<img src=\"../img/df_intro.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd91e5f",
   "metadata": {},
   "source": [
    "### Create DataFrames from a list of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39105625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(department1)\n",
    "print(employee2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf719e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da62766",
   "metadata": {},
   "outputs": [],
   "source": [
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = spark.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794410a",
   "metadata": {},
   "source": [
    "### Union two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45479bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unionDF = df1.union(df2)\n",
    "unionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f4104",
   "metadata": {},
   "source": [
    "### Explode the employees column\n",
    "\n",
    "Returns a new row for each element in the given array or map.\n",
    "Uses the default column name `col` for elements in the array and\n",
    "`key` and `value` for elements in the map unless specified otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8471ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explodeDF = unionDF.select(explode(\"employees\").alias(\"e\"))\n",
    "flattenDF = explodeDF.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "\n",
    "flattenDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f833e7",
   "metadata": {},
   "source": [
    "### Explode the employees column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74464f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "full_name = flattenDF.withColumn('full_name', concat(col(\"firstName\"), lit(\" \"), col(\"lastName\")))\n",
    "full_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ec1ca",
   "metadata": {},
   "source": [
    "### Use filter() to return the rows that match a predicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92687ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterDF = flattenDF.filter(flattenDF.firstName == \"xiangrui\").sort(flattenDF.lastName)\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "# Use `|` instead of `or`\n",
    "filterDF = flattenDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "filterDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd1f40",
   "metadata": {},
   "source": [
    "### The where() clause is equivalent to filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e40bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "whereDF = flattenDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "whereDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18e01d",
   "metadata": {},
   "source": [
    "### Replace null values with -- using DataFrame Na function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f30d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonNullDF = flattenDF.fillna(\"--\")\n",
    "nonNullDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a1279e",
   "metadata": {},
   "source": [
    "### Retrieve only rows with missing firstName or lastName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac73b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterNonNullDF = flattenDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\n",
    "filterNonNullDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f67e7",
   "metadata": {},
   "source": [
    "### Example aggregations using agg() and countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a61695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "countDistinctDF = nonNullDF.select(\"firstName\", \"lastName\")\\\n",
    "  .groupBy(\"firstName\")\\\n",
    "  .agg(countDistinct(\"lastName\").alias(\"distinct_last_names\"))\n",
    "\n",
    "countDistinctDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f539ed",
   "metadata": {},
   "source": [
    "### Compare the DataFrame and SQL query physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45033754",
   "metadata": {},
   "outputs": [],
   "source": [
    "countDistinctDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b17128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the DataFrame as a temp view so that we can query it using SQL\n",
    "nonNullDF.createOrReplaceTempView(\"databricks_df_example\")\n",
    "\n",
    "# Perform the same query as the DataFrame above and return ``explain``\n",
    "countDistinctDF_sql = spark.sql('''\n",
    "  SELECT firstName, count(distinct lastName) AS distinct_last_names\n",
    "  FROM databricks_df_example\n",
    "  GROUP BY firstName\n",
    "''')\n",
    "\n",
    "countDistinctDF_sql.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d8317",
   "metadata": {},
   "source": [
    "### Sum up all the salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "salarySumDF = nonNullDF.agg({\"salary\" : \"sum\"})\n",
    "\n",
    "salarySumDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nonNullDF.salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415007d9",
   "metadata": {},
   "source": [
    "## PySpark Join\n",
    "\n",
    "PySpark Join is used to combine two DataFrames and by chaining these you can join multiple DataFrames; it supports all basic join type operations available in traditional SQL like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN. PySpark Joins are wider transformations that involve data shuffling across the network.\n",
    "\n",
    "### PySpark Join Syntax\n",
    "PySpark SQL join has a below syntax and it can be accessed directly from DataFrame.\n",
    "\n",
    "```python\n",
    "join(self, other, on=None, how=None)\n",
    "```\n",
    "\n",
    "__join()__ operation takes parameters as below and returns DataFrame.\n",
    "\n",
    "param other: Right side of the join\n",
    "param on: a string for the join column name\n",
    "param how: default inner. Must be one of inner, cross, outer,full, full_outer, left, left_outer, right, right_outer,left_semi, and left_anti.\n",
    "You can also write Join expression by adding where() and filter() methods on DataFrame and can have Join on multiple columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99366f0f",
   "metadata": {},
   "source": [
    "Before we jump into PySpark SQL Join examples, first, let’s create an \"emp\" and \"dept\" DataFrames. here, column \"emp_id\" is unique on emp and \"dept_id\" is unique on the dept dataset’s and emp_dept_id from emp has a reference to dept_id on dept dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4698fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd4cdc",
   "metadata": {},
   "source": [
    "### PySpark Inner Join DataFrame\n",
    "\n",
    "Inner join is the default join in PySpark and it’s mostly used. This joins two datasets on key columns, where keys don’t match the rows get dropped from both datasets (emp & dept).\n",
    "\n",
    "When we apply Inner join on our datasets, It drops “emp_dept_id” 50 from “emp” and “dept_id” 30 from “dept” datasets. Below is the result of the above Join expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3db916",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id, \"inner\" ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5dde3",
   "metadata": {},
   "source": [
    "### PySpark Full Outer Join\n",
    "\n",
    "Outer a.k.a full, fullouter join returns all rows from both datasets, where join expression doesn’t match it returns null on respective record columns.\n",
    "\n",
    "From our “emp” dataset’s “emp_dept_id” with value 50 doesn’t have a record on “dept” hence dept columns have null and “dept_id” 30 doesn’t have a record in “emp” hence you see null’s on emp columns. Below is the result of the above Join expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5021f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f405b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1f7be",
   "metadata": {},
   "source": [
    "### PySpark Left Outer Join\n",
    "\n",
    "Left a.k.a Leftouter join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found.\n",
    "\n",
    "From our dataset, “emp_dept_id” 5o doesn’t have a record on “dept” dataset hence, this record contains null on “dept” columns (dept_name & dept_id). and “dept_id” 30 from “dept” dataset dropped from the results. Below is the result of the above Join expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id  ==  deptDF.dept_id,\"left\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6fde0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.join(deptDF, empDF.emp_dept_id  ==  deptDF.dept_id,\"leftouter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037ac6c",
   "metadata": {},
   "source": [
    "### Using SQL Expression\n",
    "\n",
    "Since PySpark SQL support native SQL syntax, we can also write join operations after creating temporary tables on DataFrames and use these tables on spark.sql()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "196e57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef50a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530c452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
